{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import math\n",
    "import spacy\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmartSearchEngine:\n",
    "    def __init__(self, df=None, json_folder=None, load_from_file=None):\n",
    "        \"\"\"\n",
    "        Initialize the search engine with either:\n",
    "        - df: A pandas DataFrame (original behavior)\n",
    "        - json_folder: Path to folder containing JSON files\n",
    "        - load_from_file: Path to pre-built index\n",
    "        \"\"\"\n",
    "        # Load data from JSON folder if provided\n",
    "        if json_folder is not None:\n",
    "            print(f\"Loading JSON files from: {json_folder}\")\n",
    "            self.df = self.load_json_files(json_folder)\n",
    "        elif df is not None:\n",
    "            self.df = df\n",
    "        else:\n",
    "            raise ValueError(\"Must provide either df or json_folder\")\n",
    "        \n",
    "        self.N = len(self.df)\n",
    "        \n",
    "        # Load spaCy model\n",
    "        print(\"Loading spaCy model...\")\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        \n",
    "        # Define fields to index\n",
    "        self.fields = ['Title', 'Director', 'Genres', 'Overview', 'Release_Date']\n",
    "        \n",
    "        if load_from_file:\n",
    "            # Load index from file\n",
    "            self.load_index(load_from_file)\n",
    "        else:\n",
    "            # Build index from scratch\n",
    "            self.inverted_index = defaultdict(lambda: defaultdict(list))\n",
    "            self.doc_lengths = {}\n",
    "            self.avg_doc_length = {}\n",
    "            self.directors_set = set()\n",
    "            self.genres_set = set()\n",
    "            self.title_words = set()\n",
    "            self.years_set = set()\n",
    "            \n",
    "            self.build_index()\n",
    "            self.build_recognition_dicts()\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_json_files(folder_path):\n",
    "        \"\"\"\n",
    "        Load all JSON files from a folder and combine into a DataFrame.\n",
    "        Each JSON file should contain one movie record.\n",
    "        \"\"\"\n",
    "        folder = Path(folder_path)\n",
    "        \n",
    "        if not folder.exists():\n",
    "            raise FileNotFoundError(f\"Folder not found: {folder_path}\")\n",
    "        \n",
    "        # Get all JSON files\n",
    "        json_files = list(folder.glob(\"*.json\"))\n",
    "        \n",
    "        if not json_files:\n",
    "            raise ValueError(f\"No JSON files found in: {folder_path}\")\n",
    "        \n",
    "        print(f\"Found {len(json_files)} JSON files\")\n",
    "        \n",
    "        # Load all JSON files\n",
    "        movies = []\n",
    "        for json_file in json_files:\n",
    "            try:\n",
    "                with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                    movie_data = json.load(f)\n",
    "                    movies.append(movie_data)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not load {json_file.name}: {e}\")\n",
    "        \n",
    "        print(f\"Successfully loaded {len(movies)} movies\")\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(movies)\n",
    "        \n",
    "        # Ensure required columns exist\n",
    "        required_columns = ['Title', 'Director', 'Genres', 'Overview', 'Release_Date']\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        \n",
    "        if missing_columns:\n",
    "            print(f\"Warning: Missing columns: {missing_columns}\")\n",
    "            for col in missing_columns:\n",
    "                df[col] = None\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def preprocess_text(self, text, is_date=False):\n",
    "        \"\"\"Clean and tokenize with spaCy + lemmatization\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return []\n",
    "        text = str(text).lower()\n",
    "        \n",
    "        # For dates, extract year (format: YYYY-MM-DD or just YYYY)\n",
    "        if is_date:\n",
    "            year_match = re.findall(r'\\b(?:19|20)\\d{2}\\b', text)\n",
    "            return year_match if year_match else []\n",
    "        \n",
    "        # Process with spaCy\n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        # Extract lemmas with spaCy's built-in stopwords\n",
    "        tokens = [\n",
    "            token.lemma_ \n",
    "            for token in doc \n",
    "            if not token.is_stop          # spaCy's built-in stopwords\n",
    "            and not token.is_punct        # Remove punctuation\n",
    "            and not token.is_space        # Remove whitespace\n",
    "            and len(token.lemma_) > 2     # Remove short tokens\n",
    "            and token.is_alpha            # Keep only alphabetic tokens\n",
    "        ]\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def build_index(self):\n",
    "        \"\"\"Build inverted index by field\"\"\"\n",
    "        print(\"Building inverted index...\")\n",
    "        \n",
    "        for field in self.fields:\n",
    "            self.doc_lengths[field] = {}\n",
    "            self.avg_doc_length[field] = 0\n",
    "        \n",
    "        for idx, row in self.df.iterrows():\n",
    "            for field in self.fields:\n",
    "                # Special treatment for dates\n",
    "                is_date_field = (field == 'Release_Date')\n",
    "                tokens = self.preprocess_text(row[field], is_date=is_date_field)\n",
    "                self.doc_lengths[field][idx] = len(tokens)\n",
    "                \n",
    "                term_freq = defaultdict(int)\n",
    "                for token in tokens:\n",
    "                    term_freq[token] += 1\n",
    "                \n",
    "                for term, freq in term_freq.items():\n",
    "                    self.inverted_index[field][term].append((idx, freq))\n",
    "        \n",
    "        for field in self.fields:\n",
    "            if self.doc_lengths[field]:\n",
    "                self.avg_doc_length[field] = np.mean(list(self.doc_lengths[field].values()))\n",
    "        \n",
    "        print(f\"Index built: {len(self.df)} documents indexed\")\n",
    "    \n",
    "    def build_recognition_dicts(self):\n",
    "        \"\"\"Build dictionaries to automatically recognize terms\"\"\"\n",
    "        print(\"Building recognition dictionaries...\")\n",
    "        \n",
    "        # Extract all directors\n",
    "        for director in self.df['Director'].dropna().unique():\n",
    "            tokens = self.preprocess_text(director)\n",
    "            self.directors_set.update(tokens)\n",
    "        \n",
    "        # Extract all genres\n",
    "        for genres in self.df['Genres'].dropna():\n",
    "            for genre in str(genres).split(','):\n",
    "                tokens = self.preprocess_text(genre.strip())\n",
    "                self.genres_set.update(tokens)\n",
    "        \n",
    "        # Extract important words from titles\n",
    "        for title in self.df['Title'].dropna():\n",
    "            tokens = self.preprocess_text(title)\n",
    "            self.title_words.update(tokens)\n",
    "        \n",
    "        # Extract all years from release dates\n",
    "        for date in self.df['Release_Date'].dropna():\n",
    "            years = self.preprocess_text(date, is_date=True)\n",
    "            self.years_set.update(years)\n",
    "        \n",
    "        print(f\"Unique directors: {len(self.directors_set)}\")\n",
    "        print(f\"Unique genres: {len(self.genres_set)}\")\n",
    "        print(f\"Available years: {len(self.years_set)}\")\n",
    "    \n",
    "    def classify_query_terms(self, query_tokens):\n",
    "        \"\"\"Automatically classify each query term\"\"\"\n",
    "        classified = {\n",
    "            'director': [],\n",
    "            'genre': [],\n",
    "            'title': [],\n",
    "            'year': [],\n",
    "            'general': []\n",
    "        }\n",
    "        \n",
    "        for term in query_tokens:\n",
    "            # Check if it's a year (4 digits starting with 19 or 20)\n",
    "            if re.match(r'^(?:19|20)\\d{2}$', term):\n",
    "                classified['year'].append(term)\n",
    "            # Check in which field the term appears most\n",
    "            elif term in self.directors_set:\n",
    "                classified['director'].append(term)\n",
    "            elif term in self.genres_set:\n",
    "                classified['genre'].append(term)\n",
    "            elif term in self.title_words:\n",
    "                classified['title'].append(term)\n",
    "            else:\n",
    "                # General term, search everywhere\n",
    "                classified['general'].append(term)\n",
    "        \n",
    "        return classified\n",
    "    \n",
    "    def bm25_score(self, term, doc_id, field, k1=1.5, b=0.75):\n",
    "        \"\"\"Calculate BM25 score for a term in a document\"\"\"\n",
    "        if term not in self.inverted_index[field]:\n",
    "            return 0.0\n",
    "        \n",
    "        tf = 0\n",
    "        for doc, freq in self.inverted_index[field][term]:\n",
    "            if doc == doc_id:\n",
    "                tf = freq\n",
    "                break\n",
    "        \n",
    "        if tf == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        df = len(self.inverted_index[field][term])\n",
    "        idf = math.log((self.N - df + 0.5) / (df + 0.5) + 1.0)\n",
    "        \n",
    "        doc_len = self.doc_lengths[field].get(doc_id, 0)\n",
    "        avg_len = self.avg_doc_length[field]\n",
    "        \n",
    "        if avg_len == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        norm = 1 - b + b * (doc_len / avg_len)\n",
    "        score = idf * (tf * (k1 + 1)) / (tf + k1 * norm)\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def search(self, query, top_n=10):\n",
    "        \"\"\"Smart search with automatic term classification\"\"\"\n",
    "        # Extract years BEFORE preprocessing\n",
    "        years_in_query = re.findall(r'\\b(?:19|20)\\d{2}\\b', query)\n",
    "        \n",
    "        # Tokenize query with spaCy + lemmatization\n",
    "        query_tokens = self.preprocess_text(query)\n",
    "        \n",
    "        # Add extracted years to tokens\n",
    "        query_tokens.extend(years_in_query)\n",
    "        \n",
    "        if not query_tokens:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Classify query terms\n",
    "        classified = self.classify_query_terms(query_tokens)\n",
    "        \n",
    "        # Debug: display classification\n",
    "        print(f\"\\n=== Term Classification ===\")\n",
    "        for category, terms in classified.items():\n",
    "            if terms:\n",
    "                print(f\"{category.capitalize()}: {terms}\")\n",
    "        \n",
    "        # Collect all candidate documents\n",
    "        candidate_docs = set()\n",
    "        \n",
    "        # Search terms in their corresponding fields\n",
    "        field_mapping = {\n",
    "            'director': ['Director'],\n",
    "            'genre': ['Genres'],\n",
    "            'title': ['Title'],\n",
    "            'year': ['Release_Date'],\n",
    "            'general': ['Title', 'Director', 'Genres', 'Overview']\n",
    "        }\n",
    "        \n",
    "        for category, terms in classified.items():\n",
    "            target_fields = field_mapping[category]\n",
    "            for term in terms:\n",
    "                for field in target_fields:\n",
    "                    if term in self.inverted_index[field]:\n",
    "                        for doc_id, _ in self.inverted_index[field][term]:\n",
    "                            candidate_docs.add(doc_id)\n",
    "        \n",
    "        # Calculate scores for each document\n",
    "        scores = {}\n",
    "        for doc_id in candidate_docs:\n",
    "            total_score = 0.0\n",
    "            \n",
    "            # Score for director terms\n",
    "            for term in classified['director']:\n",
    "                score = self.bm25_score(term, doc_id, 'Director')\n",
    "                total_score += score * 1.0\n",
    "            \n",
    "            # Score for genre terms\n",
    "            for term in classified['genre']:\n",
    "                score = self.bm25_score(term, doc_id, 'Genres')\n",
    "                total_score += score * 1.0\n",
    "            \n",
    "            # Score for title terms\n",
    "            for term in classified['title']:\n",
    "                score = self.bm25_score(term, doc_id, 'Title')\n",
    "                total_score += score * 1.0\n",
    "            \n",
    "            # Score for years\n",
    "            for term in classified['year']:\n",
    "                score = self.bm25_score(term, doc_id, 'Release_Date')\n",
    "                total_score += score * 1.0\n",
    "            \n",
    "            # Score for general terms\n",
    "            for term in classified['general']:\n",
    "                for field in ['Title', 'Director', 'Genres', 'Overview', 'Release_Date']:\n",
    "                    score = self.bm25_score(term, doc_id, field)\n",
    "                    total_score += score * 1.0\n",
    "            \n",
    "            scores[doc_id] = total_score\n",
    "        \n",
    "        # Sort by descending score\n",
    "        sorted_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "        \n",
    "        if not sorted_docs:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        result_indices = [doc_id for doc_id, _ in sorted_docs]\n",
    "        result_scores = [score for _, score in sorted_docs]\n",
    "        \n",
    "        results = self.df.loc[result_indices, ['Title', 'Overview', 'Genres', 'Director', 'Release_Date']].copy()\n",
    "        results['score'] = result_scores\n",
    "        \n",
    "        return results.reset_index(drop=True)\n",
    "    \n",
    "    def save_index(self, folder_path=\"../data/index_data\"):\n",
    "        \"\"\"Save inverted index and metadata to JSON\"\"\"\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "        \n",
    "        print(f\"\\nSaving index to '{folder_path}'...\")\n",
    "        \n",
    "        # Convert inverted_index to serializable format\n",
    "        serializable_index = {}\n",
    "        for field, terms_dict in self.inverted_index.items():\n",
    "            serializable_index[field] = {}\n",
    "            for term, postings in terms_dict.items():\n",
    "                serializable_index[field][term] = [[int(doc_id), int(freq)] for doc_id, freq in postings]\n",
    "        \n",
    "        # Save inverted index\n",
    "        index_path = os.path.join(folder_path, \"inverted_index.json\")\n",
    "        with open(index_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(serializable_index, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        # Save metadata\n",
    "        metadata = {\n",
    "            'N': self.N,\n",
    "            'doc_lengths': {field: {int(k): v for k, v in lengths.items()} \n",
    "                           for field, lengths in self.doc_lengths.items()},\n",
    "            'avg_doc_length': self.avg_doc_length,\n",
    "            'directors_set': list(self.directors_set),\n",
    "            'genres_set': list(self.genres_set),\n",
    "            'title_words': list(self.title_words),\n",
    "            'years_set': list(self.years_set),\n",
    "            'fields': self.fields\n",
    "        }\n",
    "        \n",
    "        metadata_path = os.path.join(folder_path, \"metadata.json\")\n",
    "        with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        # Display stats\n",
    "        index_size = os.path.getsize(index_path) / 1024\n",
    "        metadata_size = os.path.getsize(metadata_path) / 1024\n",
    "        \n",
    "        print(f\"‚úì Index saved successfully!\")\n",
    "        print(f\"  üìÅ Folder: {folder_path}\")\n",
    "        print(f\"  üìÑ inverted_index.json: {index_size:.2f} KB\")\n",
    "        print(f\"  üìÑ metadata.json: {metadata_size:.2f} KB\")\n",
    "        print(f\"  üìä Total: {index_size + metadata_size:.2f} KB\")\n",
    "    \n",
    "    def load_index(self, folder_path=\"../data\"):\n",
    "        \"\"\"Load inverted index from JSON\"\"\"\n",
    "        print(f\"\\nLoading index from '{folder_path}'...\")\n",
    "        \n",
    "        index_path = os.path.join(folder_path, \"inverted_index.json\")\n",
    "        metadata_path = os.path.join(folder_path, \"metadata.json\")\n",
    "        \n",
    "        if not os.path.exists(index_path):\n",
    "            raise FileNotFoundError(f\"File not found: {index_path}\")\n",
    "        if not os.path.exists(metadata_path):\n",
    "            raise FileNotFoundError(f\"File not found: {metadata_path}\")\n",
    "        \n",
    "        # Load inverted index\n",
    "        with open(index_path, 'r', encoding='utf-8') as f:\n",
    "            serializable_index = json.load(f)\n",
    "        \n",
    "        # Rebuild defaultdict structure with tuples\n",
    "        self.inverted_index = defaultdict(lambda: defaultdict(list))\n",
    "        for field, terms_dict in serializable_index.items():\n",
    "            for term, postings in terms_dict.items():\n",
    "                self.inverted_index[field][term] = [(doc_id, freq) for doc_id, freq in postings]\n",
    "        \n",
    "        # Load metadata\n",
    "        with open(metadata_path, 'r', encoding='utf-8') as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        self.N = metadata['N']\n",
    "        self.doc_lengths = {field: {int(k): v for k, v in lengths.items()} \n",
    "                           for field, lengths in metadata['doc_lengths'].items()}\n",
    "        self.avg_doc_length = metadata['avg_doc_length']\n",
    "        self.directors_set = set(metadata['directors_set'])\n",
    "        self.genres_set = set(metadata['genres_set'])\n",
    "        self.title_words = set(metadata['title_words'])\n",
    "        self.years_set = set(metadata['years_set'])\n",
    "        self.fields = metadata['fields']\n",
    "        \n",
    "        print(f\"‚úì Index loaded successfully!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXAMPLE 1: Creating search engine from JSON folder\n",
      "============================================================\n",
      "Loading JSON files from: ../data/Docs\n",
      "Found 3500 JSON files\n",
      "Successfully loaded 3500 movies\n",
      "Loading spaCy model...\n",
      "Building inverted index...\n",
      "Index built: 3500 documents indexed\n",
      "Building recognition dictionaries...\n",
      "Unique directors: 1905\n",
      "Unique genres: 20\n",
      "Available years: 83\n",
      "\n",
      "Saving index to '../data/index_data'...\n",
      "‚úì Index saved successfully!\n",
      "  üìÅ Folder: ../data/index_data\n",
      "  üìÑ inverted_index.json: 4788.94 KB\n",
      "  üìÑ metadata.json: 359.57 KB\n",
      "  üìä Total: 5148.51 KB\n"
     ]
    }
   ],
   "source": [
    "# ============ USAGE EXAMPLES ============\n",
    "\n",
    "# Example 1: Load from JSON folder\n",
    "print(\"=\" * 60)\n",
    "print(\"EXAMPLE 1: Creating search engine from JSON folder\")\n",
    "print(\"=\" * 60)\n",
    "engine = SmartSearchEngine(json_folder=\"../data/Docs\")\n",
    "engine.save_index(\"../data/index_data\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EXAMPLE 2: Searching\n",
      "============================================================\n",
      "\n",
      "=== Term Classification ===\n",
      "Director: ['christopher', 'nolan']\n",
      "Genre: ['action', 'movie']\n",
      "Year: ['2010']\n",
      "                                Title               Director  \\\n",
      "0                           Inception      Christopher Nolan   \n",
      "1                       Batman Begins      Christopher Nolan   \n",
      "2                     The Dark Knight      Christopher Nolan   \n",
      "3               The Dark Knight Rises      Christopher Nolan   \n",
      "4                            Insomnia      Christopher Nolan   \n",
      "5                        Interstellar      Christopher Nolan   \n",
      "6                             Memento      Christopher Nolan   \n",
      "7                        The Prestige      Christopher Nolan   \n",
      "8  Mission: Impossible - Rogue Nation  Christopher McQuarrie   \n",
      "9                  The Way of the Gun  Christopher McQuarrie   \n",
      "\n",
      "                               Genres Release_Date      score  \n",
      "0  Action, Science Fiction, Adventure   2010-07-15  15.415056  \n",
      "1                Action, Crime, Drama   2005-06-10  12.569445  \n",
      "2      Drama, Action, Crime, Thriller   2008-07-16  12.398835  \n",
      "3      Action, Crime, Drama, Thriller   2012-07-17  12.398835  \n",
      "4                     Thriller, Crime   2002-05-24  11.300340  \n",
      "5   Adventure, Drama, Science Fiction   2014-11-05  11.300340  \n",
      "6                   Mystery, Thriller   2000-10-11  11.300340  \n",
      "7     Drama, Mystery, Science Fiction   2006-10-19  11.300340  \n",
      "8                   Action, Adventure   2015-07-23   6.734147  \n",
      "9      Action, Crime, Drama, Thriller   2000-09-08   6.330187  \n"
     ]
    }
   ],
   "source": [
    "# Example 2: Search\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EXAMPLE 2: Searching\")\n",
    "print(\"=\" * 60)\n",
    "results = engine.search(\"action movies of christopher nolan 2010\", top_n=10)\n",
    "print(results[['Title', 'Director', 'Genres', 'Release_Date', 'score']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EXAMPLE 3: Loading from pre-built index\n",
      "============================================================\n",
      "Found 3500 JSON files\n",
      "Successfully loaded 3500 movies\n",
      "Loading spaCy model...\n",
      "\n",
      "Loading index from '../data/index_data'...\n",
      "‚úì Index loaded successfully!\n",
      "\n",
      "============================================================\n",
      "EXAMPLE 4: More searches\n",
      "============================================================\n",
      "\n",
      "Query: 'science fiction 2014'\n",
      "\n",
      "=== Term Classification ===\n",
      "Genre: ['science', 'fiction']\n",
      "Year: ['2014']\n",
      "              Title        Director Release_Date     score\n",
      "0          Automata     Gabe Ib√°√±ez   2014-10-09  7.013517\n",
      "1        The Signal  William Eubank   2014-03-15  7.013517\n",
      "2         The Giver   Phillip Noyce   2014-08-13  7.013517\n",
      "3              Lucy      Luc Besson   2014-07-25  7.013517\n",
      "4  Edge of Tomorrow      Doug Liman   2014-05-27  7.013517\n",
      "\n",
      "Query: 'spielberg movies'\n",
      "\n",
      "=== Term Classification ===\n",
      "Director: ['spielberg']\n",
      "Genre: ['movie']\n",
      "                        Title          Director Release_Date     score\n",
      "0   A Charlie Brown Christmas     Bill Melendez   1965-12-09  5.594881\n",
      "1  Stargate: The Ark of Truth  Robert C. Cooper   2008-03-11  4.931873\n",
      "2         Catch Me If You Can  Steven Spielberg   2002-12-16  4.885185\n",
      "3             Bridge of Spies  Steven Spielberg   2015-10-15  4.885185\n",
      "4                   War Horse  Steven Spielberg   2011-12-25  4.885185\n",
      "\n",
      "Query: 'comedy romance'\n",
      "\n",
      "=== Term Classification ===\n",
      "Genre: ['comedy', 'romance']\n",
      "                  Title      Director Release_Date     score\n",
      "0        American Pie 2   J.B. Rogers   2001-08-10  3.143193\n",
      "1      About Last Night    Steve Pink   2014-02-14  3.143193\n",
      "2  Under the Tuscan Sun  Audrey Wells   2003-09-20  3.143193\n",
      "3      Meet the Parents     Jay Roach   2000-10-06  3.143193\n",
      "4              Mallrats   Kevin Smith   1995-10-20  3.143193\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Load from pre-built index (faster for repeated use)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EXAMPLE 3: Loading from pre-built index\")\n",
    "print(\"=\" * 60)\n",
    "# Note: You need to pass the DataFrame when loading from index\n",
    "df = SmartSearchEngine.load_json_files(\"../data/Docs\")\n",
    "engine_fast = SmartSearchEngine(df=df, load_from_file=\"../data/index_data\")\n",
    "\n",
    "# Example 4: More search queries\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EXAMPLE 4: More searches\")\n",
    "print(\"=\" * 60)\n",
    "queries = [\n",
    "    \"science fiction 2014\",\n",
    "    \"spielberg movies\",\n",
    "    \"comedy romance\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    results = engine.search(query, top_n=5)\n",
    "    print(results[['Title', 'Director', 'Release_Date', 'score']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
