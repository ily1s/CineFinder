{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import re\n",
        "import math\n",
        "import spacy\n",
        "import json\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SmartSearchEngine:\n",
        "    def __init__(self, df, load_from_file=None):\n",
        "        self.df = df\n",
        "        self.N = len(df)\n",
        "        \n",
        "        # Load spaCy model\n",
        "        print(\"Loading spaCy model...\")\n",
        "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "        \n",
        "        # D√©finir les champs √† indexer\n",
        "        self.fields = ['Title', 'Director', 'Genres', 'Overview', 'Release_Date']\n",
        "        \n",
        "        if load_from_file:\n",
        "            # Charger l'index depuis un fichier\n",
        "            self.load_index(load_from_file)\n",
        "        else:\n",
        "            # Construire l'index from scratch\n",
        "            self.inverted_index = defaultdict(lambda: defaultdict(list))\n",
        "            self.doc_lengths = {}\n",
        "            self.avg_doc_length = {}\n",
        "            self.directors_set = set()\n",
        "            self.genres_set = set()\n",
        "            self.title_words = set()\n",
        "            self.years_set = set()\n",
        "            \n",
        "            self.build_index()\n",
        "            self.build_recognition_dicts()\n",
        "    \n",
        "    def preprocess_text(self, text, is_date=False):\n",
        "        \"\"\"Nettoyage et tokenisation avec spaCy + lemmatization\"\"\"\n",
        "        if pd.isna(text):\n",
        "            return []\n",
        "        text = str(text).lower()\n",
        "        \n",
        "        # Pour les dates, extraire l'ann√©e (format: YYYY-MM-DD ou juste YYYY)\n",
        "        if is_date:\n",
        "            year_match = re.findall(r'\\b(?:19|20)\\d{2}\\b', text)\n",
        "            return year_match if year_match else []\n",
        "        \n",
        "        # Process with spaCy\n",
        "        doc = self.nlp(text)\n",
        "        \n",
        "        # Extract lemmas with spaCy's built-in stopwords\n",
        "        tokens = [\n",
        "            token.lemma_ \n",
        "            for token in doc \n",
        "            if not token.is_stop          # spaCy's built-in stopwords (326 words)\n",
        "            and not token.is_punct        # Remove punctuation\n",
        "            and not token.is_space        # Remove whitespace\n",
        "            and len(token.lemma_) > 2     # Remove short tokens\n",
        "            and token.is_alpha            # Keep only alphabetic tokens\n",
        "        ]\n",
        "        \n",
        "        return tokens\n",
        "    \n",
        "    def build_index(self):\n",
        "        \"\"\"Construction de l'index invers√© par champ\"\"\"\n",
        "        print(\"Construction de l'index invers√©...\")\n",
        "        \n",
        "        for field in self.fields:\n",
        "            self.doc_lengths[field] = {}\n",
        "            self.avg_doc_length[field] = 0\n",
        "        \n",
        "        for idx, row in self.df.iterrows():\n",
        "            for field in self.fields:\n",
        "                # Traitement sp√©cial pour les dates\n",
        "                is_date_field = (field == 'Release_Date')\n",
        "                tokens = self.preprocess_text(row[field], is_date=is_date_field)\n",
        "                self.doc_lengths[field][idx] = len(tokens)\n",
        "                \n",
        "                term_freq = defaultdict(int)\n",
        "                for token in tokens:\n",
        "                    term_freq[token] += 1\n",
        "                \n",
        "                for term, freq in term_freq.items():\n",
        "                    self.inverted_index[field][term].append((idx, freq))\n",
        "        \n",
        "        for field in self.fields:\n",
        "            if self.doc_lengths[field]:\n",
        "                self.avg_doc_length[field] = np.mean(list(self.doc_lengths[field].values()))\n",
        "        \n",
        "        print(f\"Index construit : {len(self.df)} documents index√©s\")\n",
        "    \n",
        "    def build_recognition_dicts(self):\n",
        "        \"\"\"Construire des dictionnaires pour reconna√Ætre automatiquement les termes\"\"\"\n",
        "        print(\"Construction des dictionnaires de reconnaissance...\")\n",
        "        \n",
        "        # Extraire tous les r√©alisateurs\n",
        "        for director in self.df['Director'].dropna().unique():\n",
        "            tokens = self.preprocess_text(director)\n",
        "            self.directors_set.update(tokens)\n",
        "        \n",
        "        # Extraire tous les genres\n",
        "        for genres in self.df['Genres'].dropna():\n",
        "            for genre in str(genres).split(','):\n",
        "                tokens = self.preprocess_text(genre.strip())\n",
        "                self.genres_set.update(tokens)\n",
        "        \n",
        "        # Extraire mots importants des titres\n",
        "        for title in self.df['Title'].dropna():\n",
        "            tokens = self.preprocess_text(title)\n",
        "            self.title_words.update(tokens)\n",
        "        \n",
        "        # Extraire toutes les ann√©es des dates de sortie\n",
        "        for date in self.df['Release_Date'].dropna():\n",
        "            years = self.preprocess_text(date, is_date=True)\n",
        "            self.years_set.update(years)\n",
        "        \n",
        "        print(f\"R√©alisateurs uniques: {len(self.directors_set)}\")\n",
        "        print(f\"Genres uniques: {len(self.genres_set)}\")\n",
        "        print(f\"Ann√©es disponibles: {len(self.years_set)}\")\n",
        "    \n",
        "    def classify_query_terms(self, query_tokens):\n",
        "        \"\"\"Classifier automatiquement chaque terme de la requ√™te\"\"\"\n",
        "        classified = {\n",
        "            'director': [],\n",
        "            'genre': [],\n",
        "            'title': [],\n",
        "            'year': [],\n",
        "            'general': []\n",
        "        }\n",
        "        \n",
        "        for term in query_tokens:\n",
        "            # V√©rifier si c'est une ann√©e (4 chiffres commen√ßant par 19 ou 20)\n",
        "            if re.match(r'^(?:19|20)\\d{2}$', term):\n",
        "                classified['year'].append(term)\n",
        "            # V√©rifier dans quel champ le terme appara√Æt le plus\n",
        "            elif term in self.directors_set:\n",
        "                classified['director'].append(term)\n",
        "            elif term in self.genres_set:\n",
        "                classified['genre'].append(term)\n",
        "            elif term in self.title_words:\n",
        "                classified['title'].append(term)\n",
        "            else:\n",
        "                # Terme g√©n√©ral, chercher partout\n",
        "                classified['general'].append(term)\n",
        "        \n",
        "        return classified\n",
        "    \n",
        "    def bm25_score(self, term, doc_id, field, k1=1.5, b=0.75):\n",
        "        \"\"\"Calcul du score BM25 pour un terme dans un document\"\"\"\n",
        "        if term not in self.inverted_index[field]:\n",
        "            return 0.0\n",
        "        \n",
        "        tf = 0\n",
        "        for doc, freq in self.inverted_index[field][term]:\n",
        "            if doc == doc_id:\n",
        "                tf = freq\n",
        "                break\n",
        "        \n",
        "        if tf == 0:\n",
        "            return 0.0\n",
        "        \n",
        "        df = len(self.inverted_index[field][term])\n",
        "        idf = math.log((self.N - df + 0.5) / (df + 0.5) + 1.0)\n",
        "        \n",
        "        doc_len = self.doc_lengths[field].get(doc_id, 0)\n",
        "        avg_len = self.avg_doc_length[field]\n",
        "        \n",
        "        if avg_len == 0:\n",
        "            return 0.0\n",
        "        \n",
        "        norm = 1 - b + b * (doc_len / avg_len)\n",
        "        score = idf * (tf * (k1 + 1)) / (tf + k1 * norm)\n",
        "        \n",
        "        return score\n",
        "    \n",
        "    def search(self, query, top_n=10):\n",
        "        \"\"\"Recherche intelligente avec classification automatique des termes\"\"\"\n",
        "        # Extraire les ann√©es AVANT le preprocessing\n",
        "        years_in_query = re.findall(r'\\b(?:19|20)\\d{2}\\b', query)\n",
        "        \n",
        "        # Tokeniser la requ√™te avec spaCy + lemmatization\n",
        "        query_tokens = self.preprocess_text(query)\n",
        "        \n",
        "        # Ajouter les ann√©es extraites aux tokens\n",
        "        query_tokens.extend(years_in_query)\n",
        "        \n",
        "        if not query_tokens:\n",
        "            return pd.DataFrame()\n",
        "        \n",
        "        # Classifier les termes de la requ√™te\n",
        "        classified = self.classify_query_terms(query_tokens)\n",
        "        \n",
        "        # Debug: afficher la classification\n",
        "        print(f\"\\n=== Classification des termes ===\")\n",
        "        for category, terms in classified.items():\n",
        "            if terms:\n",
        "                print(f\"{category.capitalize()}: {terms}\")\n",
        "        \n",
        "        # Collecter tous les documents candidats\n",
        "        candidate_docs = set()\n",
        "        \n",
        "        # Chercher les termes dans leurs champs correspondants\n",
        "        field_mapping = {\n",
        "            'director': ['Director'],\n",
        "            'genre': ['Genres'],\n",
        "            'title': ['Title'],\n",
        "            'year': ['Release_Date'],\n",
        "            'general': ['Title', 'Director', 'Genres', 'Overview']\n",
        "        }\n",
        "        \n",
        "        for category, terms in classified.items():\n",
        "            target_fields = field_mapping[category]\n",
        "            for term in terms:\n",
        "                for field in target_fields:\n",
        "                    if term in self.inverted_index[field]:\n",
        "                        for doc_id, _ in self.inverted_index[field][term]:\n",
        "                            candidate_docs.add(doc_id)\n",
        "        \n",
        "        # Calculer les scores pour chaque document\n",
        "        scores = {}\n",
        "        for doc_id in candidate_docs:\n",
        "            total_score = 0.0\n",
        "            \n",
        "            # Score pour les termes de r√©alisateur\n",
        "            for term in classified['director']:\n",
        "                score = self.bm25_score(term, doc_id, 'Director')\n",
        "                total_score += score * 1.0\n",
        "            \n",
        "            # Score pour les termes de genre\n",
        "            for term in classified['genre']:\n",
        "                score = self.bm25_score(term, doc_id, 'Genres')\n",
        "                total_score += score * 1.0\n",
        "            \n",
        "            # Score pour les termes de titre\n",
        "            for term in classified['title']:\n",
        "                score = self.bm25_score(term, doc_id, 'Title')\n",
        "                total_score += score * 1.0\n",
        "            \n",
        "            # Score pour les ann√©es\n",
        "            for term in classified['year']:\n",
        "                score = self.bm25_score(term, doc_id, 'Release_Date')\n",
        "                total_score += score * 1.0\n",
        "            \n",
        "            # Score pour les termes g√©n√©raux\n",
        "            for term in classified['general']:\n",
        "                for field in ['Title', 'Director', 'Genres', 'Overview', 'Release_Date']:\n",
        "                    score = self.bm25_score(term, doc_id, field)\n",
        "                    total_score += score * 1.0\n",
        "            \n",
        "            scores[doc_id] = total_score\n",
        "        \n",
        "        # Trier par score d√©croissant\n",
        "        sorted_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
        "        \n",
        "        if not sorted_docs:\n",
        "            return pd.DataFrame()\n",
        "        \n",
        "        result_indices = [doc_id for doc_id, _ in sorted_docs]\n",
        "        result_scores = [score for _, score in sorted_docs]\n",
        "        \n",
        "        results = self.df.loc[result_indices, ['Title', 'Overview', 'Genres', 'Director', 'Release_Date']].copy()\n",
        "        results['score'] = result_scores\n",
        "        \n",
        "        return results.reset_index(drop=True)\n",
        "    \n",
        "    def save_index(self, folder_path=\"../data/index_data\"):\n",
        "        \"\"\"Sauvegarder l'index invers√© et les m√©tadonn√©es en JSON\"\"\"\n",
        "        # Cr√©er le dossier s'il n'existe pas\n",
        "        os.makedirs(folder_path, exist_ok=True)\n",
        "        \n",
        "        print(f\"\\nSauvegarde de l'index dans '{folder_path}'...\")\n",
        "        \n",
        "        # Convertir l'inverted_index en format s√©rialisable\n",
        "        # Structure: {field: {term: [(doc_id, freq), ...]}}\n",
        "        serializable_index = {}\n",
        "        for field, terms_dict in self.inverted_index.items():\n",
        "            serializable_index[field] = {}\n",
        "            for term, postings in terms_dict.items():\n",
        "                # Convertir les tuples en listes pour JSON\n",
        "                serializable_index[field][term] = [[int(doc_id), int(freq)] for doc_id, freq in postings]\n",
        "        \n",
        "        # Sauvegarder l'index invers√©\n",
        "        index_path = os.path.join(folder_path, \"inverted_index.json\")\n",
        "        with open(index_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(serializable_index, f, ensure_ascii=False, indent=2)\n",
        "        \n",
        "        # Sauvegarder les m√©tadonn√©es\n",
        "        metadata = {\n",
        "            'N': self.N,\n",
        "            'doc_lengths': {field: {int(k): v for k, v in lengths.items()} \n",
        "                           for field, lengths in self.doc_lengths.items()},\n",
        "            'avg_doc_length': self.avg_doc_length,\n",
        "            'directors_set': list(self.directors_set),\n",
        "            'genres_set': list(self.genres_set),\n",
        "            'title_words': list(self.title_words),\n",
        "            'years_set': list(self.years_set),\n",
        "            'fields': self.fields\n",
        "        }\n",
        "        \n",
        "        metadata_path = os.path.join(folder_path, \"metadata.json\")\n",
        "        with open(metadata_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
        "        \n",
        "        # Afficher les stats\n",
        "        index_size = os.path.getsize(index_path) / 1024\n",
        "        metadata_size = os.path.getsize(metadata_path) / 1024\n",
        "        \n",
        "        print(f\"‚úì Index sauvegard√© avec succ√®s!\")\n",
        "        print(f\"  üìÅ Dossier: {folder_path}\")\n",
        "        print(f\"  üìÑ inverted_index.json: {index_size:.2f} KB\")\n",
        "        print(f\"  üìÑ metadata.json: {metadata_size:.2f} KB\")\n",
        "        print(f\"  üìä Total: {index_size + metadata_size:.2f} KB\")\n",
        "    \n",
        "    def load_index(self, folder_path=\"../data\"):\n",
        "        \"\"\"Charger l'index invers√© depuis JSON\"\"\"\n",
        "        print(f\"\\nChargement de l'index depuis '{folder_path}'...\")\n",
        "        \n",
        "        # V√©rifier que les fichiers existent\n",
        "        index_path = os.path.join(folder_path, \"inverted_index.json\")\n",
        "        metadata_path = os.path.join(folder_path, \"metadata.json\")\n",
        "        \n",
        "        if not os.path.exists(index_path):\n",
        "            raise FileNotFoundError(f\"Fichier introuvable: {index_path}\")\n",
        "        if not os.path.exists(metadata_path):\n",
        "            raise FileNotFoundError(f\"Fichier introuvable: {metadata_path}\")\n",
        "        \n",
        "        # Charger l'index invers√©\n",
        "        with open(index_path, 'r', encoding='utf-8') as f:\n",
        "            serializable_index = json.load(f)\n",
        "        \n",
        "        # Reconstruire la structure defaultdict avec tuples\n",
        "        self.inverted_index = defaultdict(lambda: defaultdict(list))\n",
        "        for field, terms_dict in serializable_index.items():\n",
        "            for term, postings in terms_dict.items():\n",
        "                # Convertir les listes en tuples\n",
        "                self.inverted_index[field][term] = [(doc_id, freq) for doc_id, freq in postings]\n",
        "        \n",
        "        # Charger les m√©tadonn√©es\n",
        "        with open(metadata_path, 'r', encoding='utf-8') as f:\n",
        "            metadata = json.load(f)\n",
        "        \n",
        "        self.N = metadata['N']\n",
        "        self.doc_lengths = {field: {int(k): v for k, v in lengths.items()} \n",
        "                           for field, lengths in metadata['doc_lengths'].items()}\n",
        "        self.avg_doc_length = metadata['avg_doc_length']\n",
        "        self.directors_set = set(metadata['directors_set'])\n",
        "        self.genres_set = set(metadata['genres_set'])\n",
        "        self.title_words = set(metadata['title_words'])\n",
        "        self.years_set = set(metadata['years_set'])\n",
        "        self.fields = metadata['fields']\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset charg√© : 4771 films\n",
            "Colonnes disponibles : ['Title', 'Overview', 'Tagline', 'Homepage', 'Release_Date', 'Vote_Average', 'Runtime', 'Poster_Path', 'Genres', 'Keywords', 'Director', 'budget', 'revenue', 'production_companies', 'Cast']\n",
            "\n",
            "Loading spaCy model...\n",
            "Construction de l'index invers√©...\n",
            "Index construit : 4771 documents index√©s\n",
            "Construction des dictionnaires de reconnaissance...\n",
            "R√©alisateurs uniques: 2840\n",
            "Genres uniques: 21\n",
            "Ann√©es disponibles: 92\n",
            "\n",
            "Sauvegarde de l'index dans '../data/index_data'...\n",
            "‚úì Index sauvegard√© avec succ√®s!\n",
            "  üìÅ Dossier: ../data/index_data\n",
            "  üìÑ inverted_index.json: 6523.19 KB\n",
            "  üìÑ metadata.json: 491.39 KB\n",
            "  üìä Total: 7014.58 KB\n"
          ]
        }
      ],
      "source": [
        "# ============ UTILISATION AVEC VRAIES DONN√âES ============\n",
        "\n",
        "# Charger les donn√©es\n",
        "df = pd.read_csv(\"../data/cleaned_movies.csv\")\n",
        "\n",
        "print(f\"Dataset charg√© : {len(df)} films\")\n",
        "print(f\"Colonnes disponibles : {df.columns.tolist()}\\n\")\n",
        "\n",
        "# Cr√©er le moteur de recherche\n",
        "engine = SmartSearchEngine(df)\n",
        "engine.save_index(\"../data/index_data\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Classification des termes ===\n",
            "Director: ['christopher', 'nolan']\n",
            "Genre: ['action', 'movie']\n",
            "Year: ['2010']\n",
            "                                Title               Director  \\\n",
            "0                           Inception      Christopher Nolan   \n",
            "1                       Batman Begins      Christopher Nolan   \n",
            "2                     The Dark Knight      Christopher Nolan   \n",
            "3               The Dark Knight Rises      Christopher Nolan   \n",
            "4                        Interstellar      Christopher Nolan   \n",
            "5                        The Prestige      Christopher Nolan   \n",
            "6                             Memento      Christopher Nolan   \n",
            "7                            Insomnia      Christopher Nolan   \n",
            "8                      Christmas Mail         John Murlowski   \n",
            "9  Mission: Impossible - Rogue Nation  Christopher McQuarrie   \n",
            "\n",
            "                               Genres Release_Date      score  \n",
            "0  Action, Science Fiction, Adventure   2010-07-15  15.756930  \n",
            "1                Action, Crime, Drama   2005-06-10  12.911074  \n",
            "2      Drama, Action, Crime, Thriller   2008-07-16  12.722313  \n",
            "3      Action, Crime, Drama, Thriller   2012-07-17  12.722313  \n",
            "4   Adventure, Drama, Science Fiction   2014-11-05  11.538807  \n",
            "5     Drama, Mystery, Science Fiction   2006-10-19  11.538807  \n",
            "6                   Mystery, Thriller   2000-10-11  11.538807  \n",
            "7                     Thriller, Crime   2002-05-24  11.538807  \n",
            "8            Comedy, Family, TV Movie   2010-12-04   8.750636  \n",
            "9                   Action, Adventure   2015-07-23   6.792516  \n"
          ]
        }
      ],
      "source": [
        "# ============ TESTS ============\n",
        "results = engine.search(\"action movies of christopher nolan 2010\", top_n=10)\n",
        "print(results[['Title' , 'Director','Genres', 'Release_Date', 'score']])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading spaCy model...\n",
            "Construction de l'index invers√©...\n",
            "Index construit : 4771 documents index√©s\n",
            "Construction des dictionnaires de reconnaissance...\n",
            "R√©alisateurs uniques: 2840\n",
            "Genres uniques: 21\n",
            "Ann√©es disponibles: 92\n",
            "\n",
            "=== Classification des termes ===\n",
            "Director: ['tarantino']\n",
            "Genre: ['action']\n",
            "\n",
            "=== Classification des termes ===\n",
            "Director: ['nolan']\n",
            "Year: ['2010']\n",
            "\n",
            "=== Classification des termes ===\n",
            "Director: ['spielberg']\n",
            "Genre: ['adventure']\n",
            "\n",
            "=== Classification des termes ===\n",
            "Genre: ['horror']\n",
            "Year: ['2019']\n",
            "\n",
            "=== Classification des termes ===\n",
            "Genre: ['animation']\n",
            "\n",
            "=== Classification des termes ===\n",
            "Director: ['scorsese']\n",
            "Genre: ['crime']\n",
            "\n",
            "=== Classification des termes ===\n",
            "Genre: ['war']\n",
            "Title: ['star']\n",
            "\n",
            "=== Classification des termes ===\n",
            "Title: ['batman']\n",
            "\n",
            "=== Classification des termes ===\n",
            "Genre: ['comedy']\n",
            "Year: ['2015']\n",
            "\n",
            "=== Classification des termes ===\n",
            "General: ['pixar']\n",
            "\n",
            "============================================================\n",
            "R√âSULTATS DE L'√âVALUATION\n",
            "============================================================\n",
            "Accuracy:  100.00%\n",
            "Precision: 100.00%\n",
            "Recall:    100.00%\n",
            "F1-Score:  100.00%\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "class SearchEngineEvaluator:\n",
        "    \"\"\"\n",
        "    √âvaluation simple du moteur de recherche\n",
        "    Retourne juste les scores finaux\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, search_engine, df):\n",
        "        self.engine = search_engine\n",
        "        self.df = df\n",
        "    \n",
        "    def create_evaluation_dataset(self, test_queries):\n",
        "        \"\"\"Cr√©er le dataset d'√©valuation\"\"\"\n",
        "        all_results = []\n",
        "        \n",
        "        for query_info in test_queries:\n",
        "            query = query_info['query']\n",
        "            relevant_docs = set(query_info['relevant_docs'])\n",
        "            \n",
        "            results = self.engine.search(query, top_n=10)\n",
        "            \n",
        "            if results.empty:\n",
        "                continue\n",
        "            \n",
        "            for idx in results.index:\n",
        "                row = results.loc[idx]\n",
        "                all_results.append({\n",
        "                    'query': query,\n",
        "                    'doc_id': idx,\n",
        "                    'score': row['score'],\n",
        "                    'y_true': 1 if idx in relevant_docs else 0\n",
        "                })\n",
        "        \n",
        "        return pd.DataFrame(all_results)\n",
        "    \n",
        "    def find_best_threshold(self, eval_df):\n",
        "        \"\"\"Trouver le meilleur seuil pour maximiser F1\"\"\"\n",
        "        scores = eval_df['score'].values\n",
        "        thresholds = np.linspace(scores.min(), scores.max(), 10)\n",
        "        \n",
        "        best_f1 = 0\n",
        "        best_threshold = 0\n",
        "        \n",
        "        for threshold in thresholds:\n",
        "            y_pred = (eval_df['score'] >= threshold).astype(int)\n",
        "            f1 = f1_score(eval_df['y_true'], y_pred, zero_division=0)\n",
        "            \n",
        "            if f1 > best_f1:\n",
        "                best_f1 = f1\n",
        "                best_threshold = threshold\n",
        "        \n",
        "        return best_threshold\n",
        "    \n",
        "    def evaluate(self, test_queries):\n",
        "        \"\"\"\n",
        "        √âvaluation compl√®te - Retourne juste les scores\n",
        "        \n",
        "        Returns:\n",
        "            dict: {'accuracy': 0.95, 'precision': 0.92, 'recall': 0.88, 'f1_score': 0.90}\n",
        "        \"\"\"\n",
        "        # Cr√©er le dataset\n",
        "        eval_df = self.create_evaluation_dataset(test_queries)\n",
        "        \n",
        "        if eval_df.empty:\n",
        "            return {\n",
        "                'accuracy': 0.0,\n",
        "                'precision': 0.0,\n",
        "                'recall': 0.0,\n",
        "                'f1_score': 0.0,\n",
        "                'error': 'Aucun r√©sultat trouv√©'\n",
        "            }\n",
        "        \n",
        "        # Trouver le meilleur seuil\n",
        "        best_threshold = self.find_best_threshold(eval_df)\n",
        "        \n",
        "        # Pr√©dictions avec le meilleur seuil\n",
        "        eval_df['y_pred'] = (eval_df['score'] >= best_threshold).astype(int)\n",
        "        \n",
        "        y_true = eval_df['y_true'].values\n",
        "        y_pred = eval_df['y_pred'].values\n",
        "        \n",
        "        # Calculer les m√©triques\n",
        "        metrics = {\n",
        "            'accuracy': accuracy_score(y_true, y_pred),\n",
        "            'precision': precision_score(y_true, y_pred, zero_division=0),\n",
        "            'recall': recall_score(y_true, y_pred, zero_division=0),\n",
        "            'f1_score': f1_score(y_true, y_pred, zero_division=0),\n",
        "            'threshold': best_threshold\n",
        "        }\n",
        "        \n",
        "        return metrics\n",
        "\n",
        "\n",
        "# ============ HELPER POUR TROUVER LES DOCS PERTINENTS ============\n",
        "\n",
        "def find_relevant_docs(df, **criteria):\n",
        "    \"\"\"Trouve les documents pertinents selon des crit√®res\"\"\"\n",
        "    mask = pd.Series([True] * len(df), index=df.index)\n",
        "    \n",
        "    if 'title' in criteria:\n",
        "        mask &= df['Title'].str.contains(criteria['title'], case=False, na=False)\n",
        "    if 'director' in criteria:\n",
        "        mask &= df['Director'].str.contains(criteria['director'], case=False, na=False)\n",
        "    if 'genre' in criteria:\n",
        "        mask &= df['Genres'].str.contains(criteria['genre'], case=False, na=False)\n",
        "    if 'year' in criteria:\n",
        "        mask &= df['Release_Date'].str.contains(str(criteria['year']), na=False)\n",
        "    \n",
        "    return df[mask].index.tolist()\n",
        "\n",
        "\n",
        "# ============ EXEMPLE D'UTILISATION ============\n",
        "\n",
        "# 1. Charger les donn√©es\n",
        "df = pd.read_csv(\"../data/cleaned_movies.csv\")\n",
        "engine = SmartSearchEngine(df)\n",
        "\n",
        "# 2. Cr√©er les requ√™tes de test\n",
        "test_queries = [\n",
        "    {\n",
        "        'query': 'tarantino action',\n",
        "        'relevant_docs': find_relevant_docs(df, director='tarantino', genre='action')\n",
        "    },\n",
        "    {\n",
        "        'query': 'nolan 2010',\n",
        "        'relevant_docs': find_relevant_docs(df, director='nolan', year=2010)\n",
        "    },\n",
        "    {\n",
        "        'query': 'spielberg adventure',\n",
        "        'relevant_docs': find_relevant_docs(df, director='spielberg', genre='adventure')\n",
        "    },\n",
        "    {\n",
        "        'query': 'horror 2019',\n",
        "        'relevant_docs': find_relevant_docs(df, genre='horror', year=2019)\n",
        "    },\n",
        "    {\n",
        "        'query': 'animation',\n",
        "        'relevant_docs': find_relevant_docs(df, genre='animation')[:20]\n",
        "    },\n",
        "    {\n",
        "        'query': 'scorsese crime',\n",
        "        'relevant_docs': find_relevant_docs(df, director='scorsese', genre='crime')\n",
        "    },\n",
        "    {\n",
        "        'query': 'star wars',\n",
        "        'relevant_docs': find_relevant_docs(df, title='star wars')\n",
        "    },\n",
        "    {\n",
        "        'query': 'batman',\n",
        "        'relevant_docs': find_relevant_docs(df, title='batman')\n",
        "    },\n",
        "    {\n",
        "        'query': 'comedy 2015',\n",
        "        'relevant_docs': find_relevant_docs(df, genre='comedy', year=2015)\n",
        "    },\n",
        "    {\n",
        "        'query': 'pixar',\n",
        "        'relevant_docs': find_relevant_docs(df, title='pixar')[:15]\n",
        "    }\n",
        "]\n",
        "\n",
        "# 3. √âvaluer et afficher les scores\n",
        "evaluator = SearchEngineEvaluator(engine, df)\n",
        "scores = evaluator.evaluate(test_queries)\n",
        "\n",
        "# 4. Afficher les r√©sultats\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"R√âSULTATS DE L'√âVALUATION\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Accuracy:  {scores['accuracy']*100:.2f}%\")\n",
        "print(f\"Precision: {scores['precision']*100:.2f}%\")\n",
        "print(f\"Recall:    {scores['recall']*100:.2f}%\")\n",
        "print(f\"F1-Score:  {scores['f1_score']*100:.2f}%\")\n",
        "print(\"=\"*60)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
